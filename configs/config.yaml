# Hyperparameter configs

config: configs/config.yaml
cuda: True
cuda_devices: "1"

output_dir: pretrained_models/ontonotes_arabic15
experiment: tars_cross_lingual_pretrain

runs: 1
#set_seed: False
final_test: True

language_model: xlm-roberta-large
corpus: arabic_15
save_model: True
train_batch_size: 16
eval_batch_size: 16
lr: 5e-6
epochs: 10

k: 2
sampling_mode: strict
reuse_decoder_weights: False
reuse_corpus_for_weights: conll

num_negatives: one